{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "leading-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "import random\n",
    "import re\n",
    "import pronouncing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "centered-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This just makes a list of all lines of poetry, sans extra whitespace\n",
    "all_lines =[]\n",
    "for line in gzip.open(\"gutenberg-poetry-v001.ndjson.gz\"):\n",
    "    all_lines.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-obligation",
   "metadata": {},
   "source": [
    "#builds a dictionary that maps words with that rhyming part to the lines of poetry that they're found at the end of.\n",
    "from collections import defaultdict\n",
    "\n",
    "by_rhyming_part = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for line in all_lines:\n",
    "    text = line['s']\n",
    "    if not(32 < len(text) < 48): # only use lines of uniform lengths\n",
    "        continue\n",
    "        # /b indicates word boundaries\n",
    "        # w+ 'word character' alphanumeric I think\n",
    "        # W not word character \n",
    "        \n",
    "    match = re.search(r'(\\b\\w+\\b)\\W*$', text)\n",
    "    \n",
    "    if match:\n",
    "        last_word = match.group()\n",
    "        pronunciations = pronouncing.phones_for_word(last_word)\n",
    "        if len(pronunciations) > 0:\n",
    "               rhyming_part = pronouncing.rhyming_part(pronunciations[0])\n",
    "               # group by rhyming phones (for rhymes) and words (to avoid duplicate words)\n",
    "               by_rhyming_part[rhyming_part][last_word.lower()].append(text)\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anticipated-minister",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'by_rhyming_part' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5f190d1b86f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlines_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby_rhyming_part\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'by_rhyming_part' is not defined"
     ]
    }
   ],
   "source": [
    "lines_frame = pd.DataFrame(by_rhyming_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "august-dream",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "justlines = []\n",
    "for each in all_lines:\n",
    "    justlines.append(each['s'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blocked-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "no_punct = []\n",
    "for each in justlines:\n",
    "    punctless = each.strip(string.punctuation)\n",
    "    no_punct.append(punctless)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-feeding",
   "metadata": {},
   "source": [
    "### Certain words aren't being caught by my pronunciation checker. Is it 'cause they're old english or something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "english-heading",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#source_word = 'knelling'\n",
    "#print(source_word.isalpha())\n",
    "#phones = pronouncing.phones_for_word(source_word)[0] #\n",
    "#print(phones)\n",
    "#rhyming_part = pronouncing.rhyming_part(phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-coordination",
   "metadata": {},
   "source": [
    "These words make my prounouncing library dry heave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "systematic-pilot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347149"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_words =[]\n",
    "for line in no_punct:\n",
    "    text = line\n",
    "    source_word = text.split(' ')[-1]\n",
    "    try:\n",
    "        pronouncing.phones_for_word(source_word)[0]\n",
    "    except IndexError:\n",
    "        bad_words.append(text.split(' ')[-1])\n",
    "len(bad_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approximate-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62054     ever;--yet\n",
       "214019           st≈ç\n",
       "118297     Ilmarinen\n",
       "162762    Blue-bells\n",
       "280947         disio\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_words = pd.Series(bad_words)\n",
    "bad_words.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-superior",
   "metadata": {},
   "source": [
    "That's a huge number of bad words. A lot of them have punctuation at the end. I suspect a lot of those are salvagable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-chapter",
   "metadata": {},
   "source": [
    "## Analysis of original corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corrected-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This doesn't work. But it feels like it should.\n",
    "def sentimenter(line):\n",
    "    sentiment= TextBlob(line).sentiment\n",
    "    subjectivity = sentiment.subjectivity\n",
    "    return (subjectivity)\n",
    "def polarityer(line):\n",
    "    sentiment= TextBlob(line).sentiment\n",
    "    polarity = sentiment.polarity\n",
    "    return (polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "unsigned-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct_no_repeats = set(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eleven-express",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O the lovely tay was spilt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And not friendless, and not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There have I so long been staying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I is ready now ter do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An image, the foreground presents to my sight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Line\n",
       "0                     O the lovely tay was spilt\n",
       "1                    And not friendless, and not\n",
       "2              There have I so long been staying\n",
       "3                          I is ready now ter do\n",
       "4  An image, the foreground presents to my sight"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.DataFrame(no_punct_no_repeats)\n",
    "corpus['Line'] = corpus[0]\n",
    "corpus.drop([0], axis=1, inplace= True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-workstation",
   "metadata": {},
   "source": [
    "corpus['sentiment'] = corpus['Line'].apply(sentimenter)\n",
    "print('THE CODE IS RUNNING, I SWEAR.')\n",
    "corpus['polarity'] = corpus['Line'].apply(polarityer)\n",
    "corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "serial-compression",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "count_matrix = count_vect.fit_transform(corpus['Line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "peaceful-conference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2665604, 4551288)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "placed-instruction",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 391. GiB for an array with shape (52451621499,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-6c6ac048d4b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcos_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n\u001b[0m\u001b[0;32m   1189\u001b[0m                         dense_output=dense_output)\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[0;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor_axis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 391. GiB for an array with shape (52451621499,) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "cos_sim = cosine_similarity(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-encoding",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_assign = corpus.reset_index() \n",
    "def watchme(movie):\n",
    "    movie_idx = idx_assign[idx_assign['Line'] == movie].index[0]\n",
    "    sim_array = cos_sim[movie_idx]\n",
    "    array_lst = list(enumerate(sim_array))\n",
    "    sort_lst = sorted(array_lst, reverse=True, key = lambda x: x[1])\n",
    "    top_recs = sort_lst[0:40]\n",
    "    for i in top_recs:\n",
    "        print(idx_assign.loc[i[0]].Line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-excitement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "watchme(corpus['Line'].loc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-being",
   "metadata": {},
   "source": [
    "###  The below does not include cosin similarity\n",
    "\n",
    "This could be resolved by producing a list of similar lines, as above, and then selecting randomly, or the first that rhymes from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-indiana",
   "metadata": {},
   "source": [
    "# THE MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This whole thing doesn't work super reliably. Some of it has to do with the pronunciation library missing words in my corpus,\n",
    "#some of it has to do with punctuation. Working on it, but it's not super high priority.\n",
    "def limirick_machine():\n",
    "    #Finds you lines that don't need to rhyme\n",
    "    def initial_line_finder(list_of_existing):\n",
    "        rand_choice = random.choice(no_punct)\n",
    "        if rand_choice not in list_of_existing:\n",
    "            line = rand_choice\n",
    "            \n",
    "        while line.split(' ')[-1].isalpha() == False:\n",
    "            rand_choice = random.choice(no_punct)\n",
    "            if rand_choice not in list_of_existing:\n",
    "                line = rand_choice\n",
    "        \n",
    "        flag = False\n",
    "        while flag == False:            \n",
    "            try:\n",
    "                \n",
    "                word_to_rhyme = line.split(' ')[-1]\n",
    "                phones = pronouncing.phones_for_word(word_to_rhyme)\n",
    "                phones = phones[0]\n",
    "                flag= True\n",
    "                \n",
    "            except IndexError:  \n",
    "                \n",
    "                rand_choice = random.choice(no_punct)\n",
    "                while line.split(' ')[-1].isalpha() == False:\n",
    "                    rand_choice = random.choice(no_punct)\n",
    "                    if rand_choice not in list_of_existing:\n",
    "                        line = rand_choice\n",
    "                phones = pronouncing.phones_for_word(word_to_rhyme)\n",
    "                phones = phones[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        ###\n",
    "        \n",
    "        rhyming_phones = pronouncing.rhyming_part(phones)\n",
    "        return line, rhyming_phones\n",
    "    \n",
    "    # Finds lines that end with similar rhymes.\n",
    "    def secondary_line_finder(list_of_existing, phones_to_rhyme):   \n",
    "        \n",
    "        \n",
    "        rand_choice = ''\n",
    "        \n",
    "        while rand_choice in list_of_existing or rand_choice =='':\n",
    "            key = random.choice(list(by_rhyming_part[phones_to_rhyme].keys()))    \n",
    "            rand_choice = random.choice(list(by_rhyming_part[phones_to_rhyme][key]))  \n",
    "            \n",
    "        return rand_choice\n",
    "            \n",
    "\n",
    "    list_of_lines = []\n",
    "    line1, line1_phones = initial_line_finder(list_of_lines)\n",
    "    list_of_lines.append(line1)\n",
    "        \n",
    "    line2 = secondary_line_finder(list_of_lines, line1_phones)\n",
    "    list_of_lines.append(line2)\n",
    "    \n",
    "    line3, line3_phones = initial_line_finder(list_of_lines)\n",
    "    list_of_lines.append(line3)\n",
    "    \n",
    "    line4 = secondary_line_finder(list_of_lines, line3_phones)\n",
    "    list_of_lines.append(line4)\n",
    "    \n",
    "    line5 = secondary_line_finder(list_of_lines, line1_phones)\n",
    "\n",
    "    return line1, line2, line3, line4, line5\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "line1, line2, line3, line4, line5 = limirick_machine()\n",
    "print(line1)\n",
    "print(line2)\n",
    "print(line3)\n",
    "print(line4)\n",
    "print(line5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-warrant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initial set up.\n",
    "import pandas as pd\n",
    "temp_limirick_dict= {'line1':[],'line2':[],'line3':[],'line4':[],'line5':[],}\n",
    "for i in range(10):\n",
    "    line1, line2, line3, line4, line5 = limirick_machine()\n",
    "    temp_limirick_dict['line1'].append(line1)\n",
    "    temp_limirick_dict['line2'].append(line2)\n",
    "    temp_limirick_dict['line3'].append(line3)\n",
    "    temp_limirick_dict['line4'].append(line4)\n",
    "    temp_limirick_dict['line5'].append(line5)\n",
    "basic_limirick_df = pd.DataFrame(temp_limirick_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase size\n",
    "#I just run this a few times, errors don't hurt if you just run it again.\n",
    "temp_limirick_dict= {'line1':[],'line2':[],'line3':[],'line4':[],'line5':[],}\n",
    "for i in range(10):\n",
    "    line1, line2, line3, line4, line5 = limirick_machine()\n",
    "    temp_limirick_dict['line1'].append(line1)\n",
    "    temp_limirick_dict['line2'].append(line2)\n",
    "    temp_limirick_dict['line3'].append(line3)\n",
    "    temp_limirick_dict['line4'].append(line4)\n",
    "    temp_limirick_dict['line5'].append(line5)\n",
    "temp_limirick_df = pd.DataFrame(temp_limirick_dict)\n",
    "basic_limirick_df = pd.concat([basic_limirick_df, temp_limirick_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basic_limirick_df.shape)\n",
    "print(basic_limirick_df.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = basic_limirick_df['line1'].iloc[0]\n",
    "test_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-blackberry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### This doesn't work. But it feels like it should.\n",
    "def sentimenter(line):\n",
    "    sentiment= TextBlob(line).sentiment\n",
    "    subjectivity = sentiment.subjectivity\n",
    "    return (subjectivity)\n",
    "def polarityer(line):\n",
    "    sentiment= TextBlob(line).sentiment\n",
    "    polarity = sentiment.polarity\n",
    "    return (polarity)\n",
    "\n",
    "for column in basic_limirick_df:\n",
    "    basic_limirick_df[column + '_sentiment'] = basic_limirick_df[column].apply(sentimenter)\n",
    "    basic_limirick_df[column + '_polarity'] = basic_limirick_df[column].apply(polarityer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimenter1('half lying on the grass, yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_limirick_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-palace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_limirick_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "line1_negative_polarity_mask = basic_limirick_df.line1_polarity <0\n",
    "line2_negative_polarity_mask = basic_limirick_df.line2_polarity <0\n",
    "line3_negative_polarity_mask = basic_limirick_df.line3_polarity <0\n",
    "line4_negative_polarity_mask = basic_limirick_df.line4_polarity <0\n",
    "line5_negative_polarity_mask = basic_limirick_df.line5_polarity <0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gotta reset the index when I concatonate or something. But I'm just annotating on this pass.\n",
    "\n",
    "basic_limirick_df[line1_point2_polarity_mask].head()\n",
    "print(basic_limirick_df['line1'][line1_negative_polarity_mask][5])\n",
    "print(basic_limirick_df['line2'][5])\n",
    "#print(basic_limirick_df['line3'][5])\n",
    "#print(basic_limirick_df['line4'][5])\n",
    "#print(basic_limirick_df['line5'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a wordcloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"br\", \"href\"])\n",
    "textt = \" \".join(review for review in basic_limirick_df.line2)\n",
    "wordcloud = WordCloud(stopwords=stopwords).generate(textt)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('wordcloud11.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-white",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-covering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-course",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-possible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-sitting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-helen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-newton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-scanner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-input",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-dream",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-north",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-novelty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-heritage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-local",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
